{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a Decision Tree?\n",
    "In order to understand a random forest, some general background on decision trees is needed.\n",
    "\n",
    "Classification and Regression Tree models, or CART models, were introduced by `Breimen et al.` In these models, a top down approach is applied to observation data. The general idea is that given a set of observations, the following question is asked: is every target variable in this set the same (or nearly the same)? If yes, label the set of observations with the most frequent class; if no, find the best rule that splits the observations into the purest set of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How It Works\n",
    "Here is an example as applied to the [iris](https://en.wikipedia.org/wiki/Iris_flower_data_set) data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iris_decision_tree.pdf'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier,export_graphviz\n",
    "import graphviz\n",
    "\n",
    "iris_data = load_iris()\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(iris_data.data, iris_data.target)\n",
    "\n",
    "dot = export_graphviz(model, \n",
    "                        out_file=None,\n",
    "                        feature_names=iris_data.feature_names,\n",
    "                        class_names=iris_data.target_names,\n",
    "                        filled=True,\n",
    "                        impurity=None,\n",
    "                        )\n",
    "\n",
    "graph = graphviz.Source(dot)\n",
    "graph.render(\"iris_decision_tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read this tree, start from the top white node, using the first line to determine how the decision was made to split the current observations into two new nodes. Now, we can use this decision tree to classify new observations.\n",
    "\n",
    "**Observation 1**\n",
    "A flower with a petal width of 0.7 , petal length of 1.0, and a sepal width of 3.0.\n",
    "\n",
    "From the root node :\n",
    "- Is the petal width < 0.8 ?\n",
    "    - Yes -> go left.\n",
    "\n",
    "The flower is from the species setosa.\n",
    "\n",
    "**Observation 2**\n",
    "A flower with a petal width of 0.9 , petal length of 1.0, and a sepal width of 3.0.\n",
    "\n",
    "Solution\n",
    "From the root node :\n",
    "- Is the petal width less than or equal to 0.8 ?\n",
    "    - No -> go right.\n",
    "- Is the petal width less than or equal to 1.75 ?\n",
    "    - Yes -> go left.\n",
    "- Is the petal length less than or equal to 4.95 ?\n",
    "    - Yes -> go left.\n",
    "- Is the petal width less than or equal to 1.65 ?\n",
    "    - Yes -> go left\n",
    "\n",
    "The flower is from species versicolor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations to Decision Trees\n",
    "While great for producing models that are easy to understand and implement, decision trees also tend to overfit on their training data—making them perform poorly if data shown to them later don’t closely match to what they were trained on.\n",
    "\n",
    "In the special case of regression trees, they also can only predict within the range of labels that they’ve seen before, which means that they have explicit upper and lower bounds on the numbers they can produce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods\n",
    "Ensemble methods are algorithms that combine multiple algorithms into a single predictive model in order to decrease variance, decrease bias, or improve predictions.\n",
    "\n",
    "Ensemble methods are usually broken into two categories:\n",
    "- **Parallel**: An ensemble method where the models that make up the building blocks of the larger methods are generated independent of each other (i.e., they can be trained/generated as trivially parallel problems applied to the dataset).\n",
    "\n",
    "- **Sequential**: An ensemble methods where the learners are generated in a sequential order and are dependent on each other (i.e., they can only be trained one at a time, as the next model will require information from the training upstream of it).\n",
    "\n",
    "The random forest algorithm relies on a parallel ensemble method called `bagging` to generate its weak classifiers.\n",
    "\n",
    "### Bagging\n",
    "Bagging is a colloquial term for bootstrap aggregation. Bootstrap aggregation is a method that allows us to decrease the variance of an estimate by averaging multiple estimates that are measured from random subsamples of a population.\n",
    "\n",
    "### Bootstrap Sampling [Boosting]\n",
    "The first portion of bagging is the application of bootstrap sampling to obtain subsets of the data. These subsets are then fed into one model that will comprise the final ensemble method. This is a straightforward process, given a set of observation data, n observations are selected at random and with replacement to form the subsample. This subsample is what is then fed into the machine learning algorithm of choice to train the model.\n",
    "\n",
    "### Aggregation\n",
    "After all of the models have been built, their outputs must be aggregated into a single coherent prediction for the larger model. In the case of a classifier model, this is usually just a winners take all strategy—whichever category receives the most votes is the final outcome predicted. In the case of a regression problem, a simple average of predicted outcome values is used.\n",
    "\n",
    "### Feature Bagging\n",
    "Feature bagging (or the random subspace method) is a type of ensemble method that is applied to the features (columns) of a dataset instead of to the observations (rows). It is used as a method of reducing the correlation between features by training base predictors on random subsets of features instead of the complete feature space each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Random Forest\n",
    "Based on what was previously covered in decision trees and ensemble methods, it should come as little surprise as to where the random forest gets its name or how they’re constructed at a high-level, but let’s go over it anyways.\n",
    "\n",
    "A random forest is comprised of a set of decision trees, each of which is trained on a random subset of the training data. These trees predictions can then be aggregated to provide a single prediction from a series of predictions.\n",
    "\n",
    "### Building a Random Forest\n",
    "A random forest is built using the following procedure:\n",
    "- Choose the number of trees you’d like in your forest (M)\n",
    "- Choose the number of samples you’d like for each tree (n)\n",
    "- Choose the number of features you’d like in each tree (f)\n",
    "\n",
    "For each tree in M:\n",
    "- Select n samples with replacement from all observations\n",
    "- Select f features at random\n",
    "- Train a decision tree using the data set of n samples with f features\n",
    "- Save the decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How a Random Forest makes a prediction\n",
    "Given an observation (o).\n",
    "\n",
    "For each tree (t) in the model:\n",
    "- predict the outcome (p) using t applied to o\n",
    "- store p in list P\n",
    "- If the model is a classifier:\n",
    "    return max_count(p)\n",
    "- If the model is a regressor:\n",
    "    return avg(p)\n",
    "RF-predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Random Forest\n",
    "While you might be able to implement your own random forest from the ground up after going through this material, we will instead be using the scikit-learn implementation to go over how to train, make a prediction, and compare our random forest to a decision tree.\n",
    "\n",
    "Note: The iris dataset doesn’t have enough variance to make a random forest a better model than just a simple decision tree so for this example we’ll be using the breast cancer dataset instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.91      0.94      0.93        54\n",
      "      benign       0.97      0.94      0.95        89\n",
      "\n",
      "    accuracy                           0.94       143\n",
      "   macro avg       0.94      0.94      0.94       143\n",
      "weighted avg       0.94      0.94      0.94       143\n",
      "\n",
      "Random Forest Model\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   malignant       0.96      0.94      0.95        54\n",
      "      benign       0.97      0.98      0.97        89\n",
      "\n",
      "    accuracy                           0.97       143\n",
      "   macro avg       0.96      0.96      0.96       143\n",
      "weighted avg       0.97      0.97      0.96       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree     import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    " \n",
    "\n",
    "bc = load_breast_cancer()\n",
    "X = bc.data\n",
    "y = bc.target\n",
    "\n",
    "# Create our test/train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=42)\n",
    "\n",
    "\n",
    "## build our models \n",
    "decision_tree = DecisionTreeClassifier()\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "## Train the classifiers\n",
    "decision_tree.fit(X_train, y_train)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Create Predictions\n",
    "dt_pred = decision_tree.predict(X_test)\n",
    "rf_pred = random_forest.predict(X_test)\n",
    "\n",
    "# Check the performance of each model \n",
    "print('Decision Tree Model')\n",
    "print(classification_report(y_test, dt_pred, target_names=bc.target_names))\n",
    "\n",
    "print('Random Forest Model')\n",
    "print(classification_report(y_test, rf_pred, target_names=bc.target_names))\n",
    "\n",
    "#Graph our confusion matrix\n",
    "dt_cm = confusion_matrix(y_test, dt_pred)\n",
    "rf_cm = confusion_matrix(y_test, rf_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that based on common metrics of classification model performance, the random forest outperforms the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we’re able to increase the number of correctly predicted benign tumors and decrease the number of benign tumors that are predicted as malignant. By using a random forest, we can more accurately predict the state of a tumor, potentially decreasing the amount of unneeded procedures performed on patients and decreasing patient stress about their diagnosis.\n",
    "\n",
    "At this point, you’d usually start investigating `hyperparameter tuning`. This is a crucial part of the modeling process in order to ensure that your model is optimal. However, it is outside the scope of this article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "Feature importance is used to describe how much impact a feature (column) has on how the model makes decisions. They’re used in order to help guide future work by concentrating on things we are certain will have impact and perhaps ignoring things that don’t, for simplifying models and preventing overfitting by removing columns that aren’t impactful enough to be generalized, and for helping explain why a model is making the decisions it does.\n",
    "\n",
    "There are multiple methods for determining feature importance from a model. We’ll be covering how its calculated in the scikit-learn implementation. This method is popular because it's cheap to compute, does a reasonably good job of determining importance, and is already implemented.\n",
    "\n",
    "Unsurprisingly, in order to calculate the feature importance of the forest, we need to calculate the feature importance of the individual trees and then find a way to combine them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Impurity\n",
    "Gini impurity is a measure of the chance that a new observation when randomly classified would be incorrect. It is bounded between 0 and 1 (0 being impossible to be wrong and 1 being guaranteed to be wrong).\n",
    "\n",
    "Gini impurity of a node is calculated with the following equation : \n",
    "G(k)=∑i=1nP(i)×(1−P(i))\n",
    "\n",
    "where ii is a predicted category and P(i)P(i) is the probability of a record being assigned to class ii at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature\t\timportance\n",
      "petal length (cm)\t\t0.45\n",
      "petal width (cm)\t\t0.43\n",
      "sepal length (cm)\t\t0.1\n",
      "sepal width (cm)\t\t0.03\n",
      "total_importance\t\t 1.01\n"
     ]
    }
   ],
   "source": [
    "# Getting Feature Importance via sklearn \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Create our test/train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# build our model\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Train the classifier\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Get our features and weights\n",
    "feature_list = sorted(zip(map(lambda x: round(x, 2), random_forest.feature_importances_), iris.feature_names),\n",
    "             reverse=True)\n",
    "\n",
    "# Print them out\n",
    "print('feature\\t\\timportance')\n",
    "print(\"\\n\".join(['{}\\t\\t{}'.format(f,i) for i,f in feature_list]))\n",
    "print('total_importance\\t\\t',  sum([i for i,f in feature_list]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these weights, it's pretty clear that `petal shape` plays a big role in determining iris species. This might allow us to make recommendations about how we go about collecting data in the future (or not collecting data) or might give us some ideas about engineering new features around petal morphology to make our model better in the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
